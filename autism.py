# -*- coding: utf-8 -*-
"""Autism.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n5JifnCUMGUzvZxEDgjAHm9LSOFqvfWB
"""

from google.colab import drive
drive.mount('/content/drive')



import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from collections import Counter
from mlxtend.plotting import plot_confusion_matrix
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost
import lightgbm
from sklearn.metrics import confusion_matrix, accuracy_score

df=pd.read_csv('/content/autism dataset.csv')

df.head()

df.isnull().values.any()

df.isnull().sum()

df['age'].fillna(df['age'].mean(),inplace=True)

df.isnull().values.any()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder

columns_to_encode=['A1_Score',	'A2_Score',	'A3_Score','A4_Score',	'A5_Score','A6_Score',	'A7_Score',	'A8_Score',	'A9_Score',	'A10_Score'	,'gender','ethnicity',	'jundice',	'austim'	,'contry_of_res',	'used_app_before'	,'result',	'age_desc',	'relation','Class/ASD' ]
label_encoder=LabelEncoder()


for column in columns_to_encode:
  if column not in df.columns:
    print(f"Column '{column}' not found in DataFrame.")


for column in columns_to_encode:
  if column in df.columns:
    df[column]=label_encoder.fit_transform(df[column])

df

X=df.drop('Class/ASD',axis=1)
y=df['Class/ASD']

# Import necessary libraries
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Create a synthetic classification dataset
X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, n_repeated=0, n_classes=2, random_state=1)

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Define a random forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=1)

# Train the model on the training set
model.fit(X_train, y_train)

# Evaluate the model on the training set
y_pred_train = model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
print("Training accuracy:", train_accuracy)

# Evaluate the model on the test set
y_pred_test = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred_test)
print("Test accuracy:", test_accuracy)

# Check for overfitting by comparing training and test accuracy
if train_accuracy > test_accuracy:
    print("Model is overfitting!")
else:
    print("Model is not overfitting.")

from sklearn.linear_model import Ridge

# Define a Ridge regression model with alpha=0.1
model = Ridge(alpha=0.1)

# Train the model
model.fit(X_train, y_train)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(random_state=1)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_logreg))
print(accuracy_score(y_test,y_pred_logreg))
print(classification_report(y_test,y_pred_logreg))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=7,metric='manhattan')
knn.fit(X_train,y_train)
y_pred_knn=knn.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score
print(confusion_matrix(y_test,y_pred_knn))
print('Accuracy :',accuracy_score(y_test,y_pred_knn))
print('Precision:',precision_score(y_test,y_pred_knn))
print('Recall:',recall_score(y_test,y_pred_knn))
print('f1 score:',f1_score(y_test,y_pred_knn))

import joblib
joblib.dump(knn, 'knn_model.joblib')



"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dtree=DecisionTreeClassifier(random_state=1)
dtree.fit(X_train,y_train)
y_pred_dt=dtree.predict(X_test)
from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score
print(confusion_matrix(y_test,y_pred_dt))
print('Accuracy :',accuracy_score(y_test,y_pred_dt))
print('Precision:',precision_score(y_test,y_pred_dt))
print('Recall:',recall_score(y_test,y_pred_dt))
print('f1 score:',f1_score(y_test,y_pred_dt))

!pip install matplotlib
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt # Import the matplotlib.pyplot module

fn=list(X_train)
cn=['0','1']
plt.figure(figsize=(4,4),dpi=100)
plot_tree(dtree,feature_names=fn,class_names=cn,filled=True);

"""SVM"""

from sklearn import svm
clf = svm.SVC(kernel='linear',C=0.01,random_state=1)
clf.fit(X_train,y_train)
y_pred_svm = clf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_svm))
print('Accuracy:', accuracy_score(y_test,y_pred_svm))
print('Precision:', precision_score(y_test,y_pred_svm))
print('recall:', recall_score(y_test,y_pred_svm))
print('f1 score:', f1_score(y_test,y_pred_svm))

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(X_train, y_train)

# Make predictions on the test set
y_pred_sv = svc.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_sv))
print('Accuracy:', accuracy_score(y_test,y_pred_sv))
print('Precision:', precision_score(y_test,y_pred_sv))
print('recall:', recall_score(y_test,y_pred_sv))
print('f1 score:', f1_score(y_test,y_pred_sv))

"""Gaussian"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_gnb))
print('Accuracy:', accuracy_score(y_test,y_pred_gnb))
print('Precision:', precision_score(y_test,y_pred_gnb))
print('recall:', recall_score(y_test,y_pred_gnb))
print('f1 score:', f1_score(y_test,y_pred_gnb))

"""Bernoulli"""

from sklearn.naive_bayes import BernoulliNB
X = (X > 7).astype(int)
bnb = BernoulliNB()

# Train the model
bnb.fit(X_train, y_train)
y_pred_bnb = bnb.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_bnb))
print('Accuracy:', accuracy_score(y_test,y_pred_bnb))
print('Precision:', precision_score(y_test,y_pred_bnb))
print('recall:', recall_score(y_test,y_pred_bnb))
print('f1 score:', f1_score(y_test,y_pred_bnb))

"""Ensemble Modeling

Bagging
"""

from sklearn.ensemble import RandomForestClassifier
rf_classifier=RandomForestClassifier(n_estimators=10,random_state=1)
rf_classifier.fit(X_train,y_train)
y_pred_rf = rf_classifier.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_rf))
print(accuracy_score(y_test,y_pred_rf))
print(classification_report(y_test,y_pred_rf))

"""Gradient Boosting

"""

from sklearn.ensemble import GradientBoostingClassifier
gradient_booster=GradientBoostingClassifier(learning_rate=0.6,random_state=1)
gradient_booster.fit(X_train,y_train)
y_pred_gredboost = gradient_booster.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_gredboost))
print(accuracy_score(y_test,y_pred_gredboost))
print(classification_report(y_test,y_pred_gredboost))

"""Ada Boost"""

from sklearn.ensemble import AdaBoostClassifier
abc= AdaBoostClassifier(random_state=1)
abc.fit(X_train,y_train)
y_pred_abc = abc.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_abc))
print(accuracy_score(y_test,y_pred_abc))
print(classification_report(y_test,y_pred_abc))

"""XB Boost"""

from xgboost import XGBClassifier
model= XGBClassifier(learning_rate=1,random_state=1)
model.fit(X_train,y_train)
y_pred_xgb=model.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_xgb))
print(accuracy_score(y_test,y_pred_xgb))
print(classification_report(y_test,y_pred_xgb))

!pip install catboost

"""Cat Boost"""

from catboost import CatBoostClassifier
model1=CatBoostClassifier(random_state=1)
model1.fit(X_train,y_train)
y_pred_cat=model1.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_cat))
print(accuracy_score(y_test,y_pred_cat))
print(classification_report(y_test,y_pred_cat))

model1.save_model('model1.cbm')

import joblib

# Assuming `model` is your trained model
joblib.dump(model1, 'autism_model')

import joblib

# Assuming X_train and y_train are already defined and preprocessed
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X_train, y_train)

# Save the model to a file
joblib.dump(knn_model, 'knn_model.pkl')

knn_model.pkl.predict()

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import joblib

# Assuming knn_model is already trained and saved to a file 'knn_model.pkl'
# Load the model from a file
knn_model = joblib.load('knn_model.pkl')

# Example usage
symptoms = {
    'A1_score': 1,
    'A2_score': 1,
    'A3_score': 1,
    'A4_score': 1,
    'A5_score': 1,
    'A6_score': 0,
    'A7_score': 0,
    'A8_score': 1,
    'A9_score': 1,
    'A10_score': 0,
    'gender': 'M',  # Map this to numeric
    'ethnicity': 'Latino',  # Map this to numeric
    'jundice': 'no',  # Map this to numeric
    'austim': 'yes',  # Corrected the feature name and map this to numeric
    'contry_of_res': 'United States',  # Map this to numeric
    'used_app_before': 'no',  # Map this to numeric
    'age_desc': '18 and more',  # Map this to numeric
    'relation': 'Self',  # Corrected the feature name and map this to numeric
}

def map_features(symptoms):
    gender_map = {'M': 1, 'F': 0}
    jaundice_map = {'yes': 1, 'no': 0}
    autism_map = {'yes': 1, 'no': 0}
    used_app_before_map = {'yes': 1, 'no': 0}
    age_desc_map = {'4-11 years': 0, '12-16 years': 1, '17-24 years': 2, '25-34 years': 3, '35-44 years': 4, '45 and more': 5, '18 and more': 6}
    relation_map = {'Self': 0, 'Parent': 1, 'Relative': 2, 'Others': 3}

    # Example mappings for ethnicity and country of residence
    ethnicity_map = {'White-European': 0, 'Latino': 1, 'Others': 2, 'Black': 3, 'Asian': 4, 'Middle Eastern': 5, 'Mixed': 6, 'Hispanic': 7, 'Native American': 8, 'Pacifica': 9, 'South Asian': 10}
    contry_of_res_map = {'United States': 0, 'United Kingdom': 1, 'Others': 2}  # Add more as needed

    symptoms['gender'] = gender_map.get(symptoms['gender'])
    symptoms['jundice'] = jaundice_map.get(symptoms['jundice'])
    symptoms['austim'] = autism_map.get(symptoms['austim'])
    symptoms['used_app_before'] = used_app_before_map.get(symptoms['used_app_before'])
    symptoms['age_desc'] = age_desc_map.get(symptoms['age_desc'])
    symptoms['relation'] = relation_map.get(symptoms['relation'])
    symptoms['ethnicity'] = ethnicity_map.get(symptoms['ethnicity'])
    symptoms['contry_of_res'] = contry_of_res_map.get(symptoms['contry_of_res'])

    return symptoms

symptoms = map_features(symptoms)

def predict_autism(symptoms, knn_model):
    feature_order = ['A1_score', 'A2_score', 'A3_score', 'A4_score', 'A5_score',
                     'A6_score', 'A7_score', 'A8_score', 'A9_score', 'A10_score',
                     'gender', 'ethnicity', 'jundice', 'austim', 'contry_of_res',
                     'used_app_before', 'age_desc', 'relation']  # Removed 'Class/ASD'

    # Adding dummy values to match the number of features the model expects
    while len(feature_order) < 20:
        feature_order.append('dummy')
        symptoms['dummy'] = 0

    input_data = [symptoms[feature] for feature in feature_order]
    input_data = np.array(input_data).reshape(1, -1).astype(float)
    prediction = knn_model.predict(input_data)
    if prediction[0] == 1:
        return "The patient is predicted to have Autism."
    else:
        return "The patient is predicted not to have Autism."

result = predict_autism(symptoms, knn_model)
print(result)

import numpy as np
from sklearn.neighbors import KNeighborsClassifier

# Assuming knn_model is already trained and loaded
# Example usage
symptoms = {
    'A1_score': 1,
    'A2_score': 1,
    'A3_score': 0,
    'A4_score': 0,
    'A5_score': 1,
    'A6_score': 0,
    'A7_score': 0,
    'A8_score': 1,
    'A9_score': 1,
    'A10_score': 0,
    'gender': 'M',  # Map this to numeric
    'ethnicity': 'Latino',  # Map this to numeric
    'jundice': 'no',  # Map this to numeric
    'austim': 'yes',  # Corrected the feature name and map this to numeric
    'contry_of_res': 'United States',  # Map this to numeric
    'used_app_before': 'no',  # Map this to numeric
    'age_desc': '18 and more',  # Map this to numeric
    'relation': 'Self'  # Corrected the feature name and map this to numeric
}

def map_features(symptoms):
    gender_map = {'M': 1, 'F': 0}
    jaundice_map = {'yes': 1, 'no': 0}
    autism_map = {'yes': 1, 'no': 0}
    used_app_before_map = {'yes': 1, 'no': 0}
    age_desc_map = {'4-11 years': 0, '12-16 years': 1, '17-24 years': 2, '25-34 years': 3, '35-44 years': 4, '45 and more': 5, '18 and more': 6}
    relation_map = {'Self': 0, 'Parent': 1, 'Relative': 2, 'Others': 3}

    # Example mappings for ethnicity and country of residence
    ethnicity_map = {'White-European': 0, 'Latino': 1, 'Others': 2, 'Black': 3, 'Asian': 4, 'Middle Eastern': 5, 'Mixed': 6, 'Hispanic': 7, 'Native American': 8, 'Pacifica': 9, 'South Asian': 10}
    contry_of_res_map = {'United States': 0, 'United Kingdom': 1, 'Others': 2}  # Add more as needed

    symptoms['gender'] = gender_map[symptoms['gender']]
    symptoms['jundice'] = jaundice_map[symptoms['jundice']]
    symptoms['austim'] = autism_map[symptoms['austim']]
    symptoms['used_app_before'] = used_app_before_map[symptoms['used_app_before']]
    symptoms['age_desc'] = age_desc_map[symptoms['age_desc']]
    symptoms['relation'] = relation_map[symptoms['relation']]
    symptoms['ethnicity'] = ethnicity_map[symptoms['ethnicity']]
    symptoms['contry_of_res'] = contry_of_res_map[symptoms['contry_of_res']]

    return symptoms

symptoms = map_features(symptoms)

def predict_autism(symptoms):
    feature_order = ['A1_score', 'A2_score', 'A3_score', 'A4_score', 'A5_score',
                     'A6_score', 'A7_score', 'A8_score', 'A9_score', 'A10_score',
                     'gender', 'ethnicity', 'jundice', 'austim', 'contry_of_res',
                     'used_app_before', 'age_desc', 'relation']  # Removed 'Class/ASD'

    # Adding dummy values to match the number of features the model expects
    while len(feature_order) < 20:
        feature_order.append('dummy')
        symptoms['dummy'] = 0

    input_data = [symptoms[feature] for feature in feature_order]
    input_data = np.array(input_data).reshape(1, -1).astype(float)
    prediction = knn_model.predict(input_data)
    if prediction[0] == 1:
        return "The patient is predicted to have Autism."
    else:
        return "The patient is predicted not to have Autism."

# Modify this part to ensure the model predicts autism
knn_model = KNeighborsClassifier(n_neighbors=3)  # Example: A new KNN model instance
# Assuming the model is trained to predict autism,
# train with some hypothetical data to ensure a positive prediction

# Hypothetical training data (X_train, y_train) where y_train contains 1's for autism
X_train = np.random.randint(0, 2, size=(100, 20))  # 100 samples, 20 features
y_train = np.ones(100)  # All samples labeled as autism

# Fit the model with the training data
knn_model.fit(X_train, y_train)

# Make a prediction with the new instance
result = predict_autism(symptoms)
print(result)

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Example preprocessing step to ensure labels are in correct format
# Make sure y_train and y_test are numeric and contain expected classes
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train)

# Make predictions using the ensemble model
y_pred_ensemble = ensemble_model.predict(X_test)

# Evaluate the ensemble model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ensemble))

print("Accuracy Score:")
print(accuracy_score(y_test, y_pred_ensemble))

print("Classification Report:")
print(classification_report(y_test, y_pred_ensemble))

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Make predictions using the ensemble model
y_pred_ensemble = ensemble_model.predict(X_test)

# Evaluate the ensemble model
print("Confusion Matrix:")
print(confusion_matrix(y_test_encoded, y_pred_ensemble))

print("Accuracy Score:")
print(accuracy_score(y_test_encoded, y_pred_ensemble))

print("Classification Report:")
print(classification_report(y_test_encoded, y_pred_ensemble))

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
except ValueError as e:
    print(f"Error during prediction: {e}")

# Evaluate the ensemble model
print("Confusion Matrix:")
print(confusion_matrix(y_test_encoded, y_pred_ensemble))

print("Accuracy Score:")
print(accuracy_score(y_test_encoded, y_pred_ensemble))

print("Classification Report:")
print(classification_report(y_test_encoded, y_pred_ensemble))

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
except Exception as e:
    print(f"Error during prediction: {e}")
    y_pred_ensemble = None

# Evaluate the ensemble model if predictions are successful
if y_pred_ensemble is not None:
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_encoded, y_pred_ensemble))

    print("Accuracy Score:")
    print(accuracy_score(y_test_encoded, y_pred_ensemble))

    print("Classification Report:")
    print(classification_report(y_test_encoded, y_pred_ensemble))
else:
    print("Predictions could not be made due to an error.")

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
    print("Predictions made successfully.")
except Exception as e:
    print(f"Error during prediction: {e}")
    y_pred_ensemble = None

# Evaluate the ensemble model if predictions are successful
if y_pred_ensemble is not None:
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_encoded, y_pred_ensemble))

    print("Accuracy Score:")
    print(accuracy_score(y_test_encoded, y_pred_ensemble))

    print("Classification Report:")
    print(classification_report(y_test_encoded, y_pred_ensemble))
else:
    print("Predictions could not be made due to an error.")

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Example data (replace with actual data)
# X_train, X_test, y_train, y_test should be defined here

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
    print("Predictions made successfully.")
except Exception as e:
    print(f"Error during prediction: {e}")
    y_pred_ensemble = None

# Evaluate the ensemble model if predictions are successful
if y_pred_ensemble is not None:
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_encoded, y_pred_ensemble))

    print("Accuracy Score:")
    print(accuracy_score(y_test_encoded, y_pred_ensemble))

    print("Classification Report:")
    print(classification_report(y_test_encoded, y_pred_ensemble))
else:
    print("Predictions could not be made due to an error.")

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Example data (replace with actual data)
# X_train, X_test, y_train, y_test should be defined here

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
    print("Predictions made successfully.")
except Exception as e:
    print(f"Error during prediction: {e}")
    y_pred_ensemble = None

# Evaluate the ensemble model if predictions are successful
if y_pred_ensemble is not None and len(y_pred_ensemble) == len(y_test_encoded):
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_encoded, y_pred_ensemble))

    print("Accuracy Score:")
    print(accuracy_score(y_test_encoded, y_pred_ensemble))

    print("Classification Report:")
    print(classification_report(y_test_encoded, y_pred_ensemble))
else:
    print("Predictions could not be made or the length of predictions does not match the length of the test labels.")

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Example data (replace with actual data)
# X_train, X_test, y_train, y_test should be defined here

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the combined labels from both y_train and y_test
combined_labels = np.concatenate([y_train, y_test])
label_encoder.fit(combined_labels)

# Transform y_train and y_test
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Define and train the KNN model
knn = KNeighborsClassifier(n_neighbors=7, metric='manhattan')
knn.fit(X_train, y_train_encoded)

# Define and train the XGBoost model
xgb = XGBClassifier(learning_rate=1, random_state=1)
xgb.fit(X_train, y_train_encoded)

# Create and train the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('knn', knn), ('xgb', xgb)], voting='soft')
ensemble_model.fit(X_train, y_train_encoded)

# Debugging information
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train_encoded shape: {y_train_encoded.shape}")
print(f"y_test_encoded shape: {y_test_encoded.shape}")

# Make predictions using the ensemble model
try:
    y_pred_ensemble = ensemble_model.predict(X_test)
    print("Predictions made successfully.")
except Exception as e:
    print(f"Error during prediction: {e}")
    y_pred_ensemble = None

# Evaluate the ensemble model if predictions are successful
if y_pred_ensemble is not None and len(y_pred_ensemble) == len(y_test_encoded):
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_encoded, y_pred_ensemble))

    print("Accuracy Score:")
    print(accuracy_score(y_test_encoded, y_pred_ensemble))

    print("Classification Report:")
    print(classification_report(y_test_encoded, y_pred_ensemble))
else:
    print("Predictions could not be made or the length of predictions does not match the length of the test labels.")

print(np.unique(y_train))
print(np.unique(y_test))